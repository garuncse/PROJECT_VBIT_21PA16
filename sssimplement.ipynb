{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPT5FfiFFfFHCmVPiBckCWR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sOTfMxi1IqyZ","executionInfo":{"status":"ok","timestamp":1733994934567,"user_tz":-330,"elapsed":10641,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}},"outputId":"1bb5b19f-45d3-43b8-a4c7-2c3c7fb925f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n","Collecting clip-by-openai\n","  Downloading clip_by_openai-1.1-py3-none-any.whl.metadata (369 bytes)\n","Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Collecting ftfy (from clip-by-openai)\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip-by-openai) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip-by-openai) (4.66.6)\n","INFO: pip is looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n","Collecting clip-by-openai\n","  Downloading clip_by_openai-1.0.1-py3-none-any.whl.metadata (407 bytes)\n","  Downloading clip_by_openai-0.1.1.5-py3-none-any.whl.metadata (8.6 kB)\n","  Downloading clip_by_openai-0.1.1.4-py3-none-any.whl.metadata (8.6 kB)\n","  Downloading clip_by_openai-0.1.1.3-py3-none-any.whl.metadata (8.7 kB)\n","  Downloading clip_by_openai-0.1.1.2-py3-none-any.whl.metadata (9.0 kB)\n","  Downloading clip_by_openai-0.1.1-py3-none-any.whl.metadata (9.0 kB)\n","  Downloading clip_by_openai-0.1.0-py3-none-any.whl.metadata (9.0 kB)\n","INFO: pip is still looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchvision\n","  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","  Downloading torchvision-0.20.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n","Collecting torch\n","  Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n","  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.1.0 (from torch)\n","  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.19.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n","Collecting torch\n","  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==3.0.0 (from torch)\n","  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n","Collecting torchvision\n","  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n","Collecting torch\n","  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Collecting torch\n","  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting triton==2.3.1 (from torch)\n","  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Collecting torch\n","  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","Collecting triton==2.3.0 (from torch)\n","  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Collecting torch\n","  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting triton==2.2.0 (from torch)\n","  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.17.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Collecting torch\n","  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.32.3)\n","Collecting torch\n","  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Collecting torch\n","  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting nvidia-nccl-cu12==2.18.1 (from torch)\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting triton==2.1.0 (from torch)\n","  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.16.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Collecting torch\n","  Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Collecting torch\n","  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n","Collecting torch\n","  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n","Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch)\n","  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu11==11.10.3.66 (from torch)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n","  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu11==10.2.10.91 (from torch)\n","  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch)\n","  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch)\n","  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu11==2.14.3 (from torch)\n","  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu11==11.7.91 (from torch)\n","  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.0.0 (from torch)\n","  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (75.1.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.45.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.30.5)\n","Collecting lit (from triton==2.0.0->torch)\n","  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n","Collecting torch\n","  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n","Collecting torch\n","  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.14.0-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n","Collecting torch\n","  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (23 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\n","Collecting torch\n","  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\n","Collecting torch\n","  Downloading torch-1.12.0-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.12.0-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\n","Collecting torch\n","  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n","\u001b[31mERROR: Cannot install clip-by-openai==0.1.0, clip-by-openai==0.1.1, clip-by-openai==0.1.1.2, clip-by-openai==0.1.1.3, clip-by-openai==0.1.1.4, clip-by-openai==0.1.1.5, clip-by-openai==1.0.1, clip-by-openai==1.1, torch and torchvision==0.12.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n","\u001b[0m\n","The conflict is caused by:\n","    The user requested torch\n","    torchvision 0.12.0 depends on torch==1.11.0\n","    clip-by-openai 1.1 depends on torch<1.7.2 and >=1.7.1\n","    The user requested torch\n","    torchvision 0.12.0 depends on torch==1.11.0\n","    clip-by-openai 1.0.1 depends on torch<1.7.2 and >=1.7.1\n","    The user requested torch\n","    torchvision 0.12.0 depends on torch==1.11.0\n","    clip-by-openai 0.1.1.5 depends on torch==1.7.1\n","    The user requested torch\n","    torchvision 0.12.0 depends on torch==1.11.0\n","    clip-by-openai 0.1.1.4 depends on torch==1.7.1\n","    The user requested torch\n","    torchvision 0.12.0 depends on torch==1.11.0\n","    clip-by-openai 0.1.1.3 depends on torch==1.7.1\n","    The user requested torch\n","    torchvision 0.12.0 depends on torch==1.11.0\n","    clip-by-openai 0.1.1.2 depends on torch==1.7.1\n","    The user requested torch\n","    torchvision 0.12.0 depends on torch==1.11.0\n","    clip-by-openai 0.1.1 depends on torch==1.7.1\n","    The user requested torch\n","    torchvision 0.12.0 depends on torch==1.11.0\n","    clip-by-openai 0.1.0 depends on torch==1.7.1\n","\n","To fix this you could try to:\n","1. loosen the range of package versions you've specified\n","2. remove package versions to allow pip to attempt to solve the dependency conflict\n","\n","\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n","\u001b[0mCollecting ultralytics\n","  Downloading ultralytics-8.3.49-py3-none-any.whl.metadata (35 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.13-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.49-py3-none-any.whl (898 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.13-py3-none-any.whl (26 kB)\n","Installing collected packages: ultralytics-thop, ultralytics\n","Successfully installed ultralytics-8.3.49 ultralytics-thop-2.0.13\n"]}],"source":["!pip install opencv-python-headless matplotlib pyyaml torch torchvision clip-by-openai pytorch-lightning\n","!pip install -U ultralytics\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zuLQG5VJycn","executionInfo":{"status":"ok","timestamp":1733995002632,"user_tz":-330,"elapsed":25532,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}},"outputId":"715bd30a-7067-45a4-c934-0285052aa15c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import torch\n","import yaml\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from ultralytics import YOLO\n","from torchvision import transforms\n","from torch.nn import functional as F\n"],"metadata":{"id":"zZdg0d6aJygP","executionInfo":{"status":"ok","timestamp":1733996374596,"user_tz":-330,"elapsed":6615,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install ultralytics\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rapxEFgSPNgG","executionInfo":{"status":"ok","timestamp":1733996383147,"user_tz":-330,"elapsed":2873,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}},"outputId":"972588c5-cb63-49e4-e81f-6354443f24f1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.49)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n","Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.13)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"]}]},{"cell_type":"code","source":["from ultralytics import YOLO\n","import cv2\n","import os\n"],"metadata":{"id":"OEQ9-p86PNlD","executionInfo":{"status":"ok","timestamp":1733996393428,"user_tz":-330,"elapsed":387,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Load YOLOv8n (nano) model\n","yolo = YOLO('yolov8n.pt')  # Pretrained YOLOv8 Nano model\n"],"metadata":{"id":"B2hEsKb5PNo8","executionInfo":{"status":"ok","timestamp":1733996406992,"user_tz":-330,"elapsed":335,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"uuzM4eMDPNtr","executionInfo":{"status":"ok","timestamp":1733996484958,"user_tz":-330,"elapsed":66689,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}},"outputId":"78019bdd-b7bc-486c-a176-3a3b66e0082d"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-f8c6fd6e-1041-48ba-ad58-e6bcad03e581\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-f8c6fd6e-1041-48ba-ad58-e6bcad03e581\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving 105.mp4 to 105.mp4\n"]}]},{"cell_type":"code","source":["# Perform object detection on the video\n","results = yolo(\"/content/105.mp4\")\n","\n","# Ensure the detections directory exists\n","save_dir = 'detections/'\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# Iterate over each result and save\n","for i, result in enumerate(results):\n","    save_path = os.path.join(save_dir, f\"frame_{i}.jpg\")  # Save each frame's detection\n","    result.plot()  # Plot the result on the frame\n","    result.save(save_path)  # Save the result frame to the path\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8rKtsOtPsW9","executionInfo":{"status":"ok","timestamp":1733996662413,"user_tz":-330,"elapsed":4026,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}},"outputId":"c1d70237-1455-40c7-86c6-cfe766c4777f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","WARNING ⚠️ inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n","errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n","\n","Example:\n","    results = model(source=..., stream=True)  # generator of Results objects\n","    for r in results:\n","        boxes = r.boxes  # Boxes object for bbox outputs\n","        masks = r.masks  # Masks object for segment masks outputs\n","        probs = r.probs  # Class probabilities for classification outputs\n","\n","video 1/1 (frame 1/96) /content/105.mp4: 384x640 4 persons, 9.4ms\n","video 1/1 (frame 2/96) /content/105.mp4: 384x640 4 persons, 9.2ms\n","video 1/1 (frame 3/96) /content/105.mp4: 384x640 4 persons, 8.0ms\n","video 1/1 (frame 4/96) /content/105.mp4: 384x640 4 persons, 9.6ms\n","video 1/1 (frame 5/96) /content/105.mp4: 384x640 3 persons, 7.1ms\n","video 1/1 (frame 6/96) /content/105.mp4: 384x640 3 persons, 1 chair, 11.1ms\n","video 1/1 (frame 7/96) /content/105.mp4: 384x640 3 persons, 1 remote, 8.5ms\n","video 1/1 (frame 8/96) /content/105.mp4: 384x640 2 persons, 9.2ms\n","video 1/1 (frame 9/96) /content/105.mp4: 384x640 3 persons, 11.8ms\n","video 1/1 (frame 10/96) /content/105.mp4: 384x640 2 persons, 1 dog, 1 remote, 7.2ms\n","video 1/1 (frame 11/96) /content/105.mp4: 384x640 3 persons, 9.2ms\n","video 1/1 (frame 12/96) /content/105.mp4: 384x640 3 persons, 8.2ms\n","video 1/1 (frame 13/96) /content/105.mp4: 384x640 4 persons, 7.7ms\n","video 1/1 (frame 14/96) /content/105.mp4: 384x640 4 persons, 1 remote, 10.1ms\n","video 1/1 (frame 15/96) /content/105.mp4: 384x640 4 persons, 1 remote, 9.0ms\n","video 1/1 (frame 16/96) /content/105.mp4: 384x640 4 persons, 1 remote, 7.9ms\n","video 1/1 (frame 17/96) /content/105.mp4: 384x640 4 persons, 1 remote, 7.4ms\n","video 1/1 (frame 18/96) /content/105.mp4: 384x640 5 persons, 7.0ms\n","video 1/1 (frame 19/96) /content/105.mp4: 384x640 4 persons, 12.0ms\n","video 1/1 (frame 20/96) /content/105.mp4: 384x640 4 persons, 9.7ms\n","video 1/1 (frame 21/96) /content/105.mp4: 384x640 5 persons, 6.8ms\n","video 1/1 (frame 22/96) /content/105.mp4: 384x640 4 persons, 7.7ms\n","video 1/1 (frame 23/96) /content/105.mp4: 384x640 3 persons, 9.1ms\n","video 1/1 (frame 24/96) /content/105.mp4: 384x640 4 persons, 22.6ms\n","video 1/1 (frame 25/96) /content/105.mp4: 384x640 5 persons, 8.0ms\n","video 1/1 (frame 26/96) /content/105.mp4: 384x640 5 persons, 7.8ms\n","video 1/1 (frame 27/96) /content/105.mp4: 384x640 5 persons, 9.5ms\n","video 1/1 (frame 28/96) /content/105.mp4: 384x640 3 persons, 8.0ms\n","video 1/1 (frame 29/96) /content/105.mp4: 384x640 4 persons, 7.2ms\n","video 1/1 (frame 30/96) /content/105.mp4: 384x640 4 persons, 6.6ms\n","video 1/1 (frame 31/96) /content/105.mp4: 384x640 3 persons, 1 tv, 9.1ms\n","video 1/1 (frame 32/96) /content/105.mp4: 384x640 3 persons, 1 tv, 9.1ms\n","video 1/1 (frame 33/96) /content/105.mp4: 384x640 4 persons, 1 tv, 7.3ms\n","video 1/1 (frame 34/96) /content/105.mp4: 384x640 4 persons, 9.4ms\n","video 1/1 (frame 35/96) /content/105.mp4: 384x640 3 persons, 7.6ms\n","video 1/1 (frame 36/96) /content/105.mp4: 384x640 3 persons, 6.7ms\n","video 1/1 (frame 37/96) /content/105.mp4: 384x640 3 persons, 7.5ms\n","video 1/1 (frame 38/96) /content/105.mp4: 384x640 3 persons, 9.3ms\n","video 1/1 (frame 39/96) /content/105.mp4: 384x640 4 persons, 7.2ms\n","video 1/1 (frame 40/96) /content/105.mp4: 384x640 4 persons, 1 tv, 7.8ms\n","video 1/1 (frame 41/96) /content/105.mp4: 384x640 3 persons, 1 tv, 7.7ms\n","video 1/1 (frame 42/96) /content/105.mp4: 384x640 3 persons, 1 tv, 9.0ms\n","video 1/1 (frame 43/96) /content/105.mp4: 384x640 4 persons, 1 tv, 8.5ms\n","video 1/1 (frame 44/96) /content/105.mp4: 384x640 4 persons, 1 tv, 8.5ms\n","video 1/1 (frame 45/96) /content/105.mp4: 384x640 4 persons, 1 tv, 7.3ms\n","video 1/1 (frame 46/96) /content/105.mp4: 384x640 4 persons, 1 tv, 9.0ms\n","video 1/1 (frame 47/96) /content/105.mp4: 384x640 4 persons, 1 tv, 9.6ms\n","video 1/1 (frame 48/96) /content/105.mp4: 384x640 3 persons, 1 tv, 9.5ms\n","video 1/1 (frame 49/96) /content/105.mp4: 384x640 3 persons, 1 tv, 8.2ms\n","video 1/1 (frame 50/96) /content/105.mp4: 384x640 3 persons, 1 tv, 9.9ms\n","video 1/1 (frame 51/96) /content/105.mp4: 384x640 3 persons, 1 tv, 9.7ms\n","video 1/1 (frame 52/96) /content/105.mp4: 384x640 3 persons, 1 tv, 11.4ms\n","video 1/1 (frame 53/96) /content/105.mp4: 384x640 3 persons, 12.5ms\n","video 1/1 (frame 54/96) /content/105.mp4: 384x640 3 persons, 1 tv, 11.0ms\n","video 1/1 (frame 55/96) /content/105.mp4: 384x640 3 persons, 1 remote, 12.6ms\n","video 1/1 (frame 56/96) /content/105.mp4: 384x640 2 persons, 1 cat, 1 tv, 11.0ms\n","video 1/1 (frame 57/96) /content/105.mp4: 384x640 2 persons, 1 dog, 1 remote, 10.2ms\n","video 1/1 (frame 58/96) /content/105.mp4: 384x640 2 persons, 1 dog, 11.2ms\n","video 1/1 (frame 59/96) /content/105.mp4: 384x640 2 persons, 1 cat, 1 dog, 9.1ms\n","video 1/1 (frame 60/96) /content/105.mp4: 384x640 2 persons, 1 dog, 9.3ms\n","video 1/1 (frame 61/96) /content/105.mp4: 384x640 2 persons, 1 dog, 8.4ms\n","video 1/1 (frame 62/96) /content/105.mp4: 384x640 2 persons, 1 dog, 8.3ms\n","video 1/1 (frame 63/96) /content/105.mp4: 384x640 3 persons, 7.9ms\n","video 1/1 (frame 64/96) /content/105.mp4: 384x640 2 persons, 1 dog, 10.7ms\n","video 1/1 (frame 65/96) /content/105.mp4: 384x640 3 persons, 1 dog, 13.3ms\n","video 1/1 (frame 66/96) /content/105.mp4: 384x640 3 persons, 6.8ms\n","video 1/1 (frame 67/96) /content/105.mp4: 384x640 3 persons, 7.0ms\n","video 1/1 (frame 68/96) /content/105.mp4: 384x640 4 persons, 7.8ms\n","video 1/1 (frame 69/96) /content/105.mp4: 384x640 4 persons, 9.5ms\n","video 1/1 (frame 70/96) /content/105.mp4: 384x640 4 persons, 6.7ms\n","video 1/1 (frame 71/96) /content/105.mp4: 384x640 4 persons, 9.0ms\n","video 1/1 (frame 72/96) /content/105.mp4: 384x640 4 persons, 8.0ms\n","video 1/1 (frame 73/96) /content/105.mp4: 384x640 4 persons, 6.7ms\n","video 1/1 (frame 74/96) /content/105.mp4: 384x640 4 persons, 7.5ms\n","video 1/1 (frame 75/96) /content/105.mp4: 384x640 4 persons, 9.0ms\n","video 1/1 (frame 76/96) /content/105.mp4: 384x640 4 persons, 8.4ms\n","video 1/1 (frame 77/96) /content/105.mp4: 384x640 3 persons, 6.6ms\n","video 1/1 (frame 78/96) /content/105.mp4: 384x640 3 persons, 7.7ms\n","video 1/1 (frame 79/96) /content/105.mp4: 384x640 3 persons, 8.8ms\n","video 1/1 (frame 80/96) /content/105.mp4: 384x640 3 persons, 8.7ms\n","video 1/1 (frame 81/96) /content/105.mp4: 384x640 3 persons, 7.5ms\n","video 1/1 (frame 82/96) /content/105.mp4: 384x640 3 persons, 8.2ms\n","video 1/1 (frame 83/96) /content/105.mp4: 384x640 3 persons, 8.9ms\n","video 1/1 (frame 84/96) /content/105.mp4: 384x640 4 persons, 10.4ms\n","video 1/1 (frame 85/96) /content/105.mp4: 384x640 3 persons, 6.4ms\n","video 1/1 (frame 86/96) /content/105.mp4: 384x640 3 persons, 9.2ms\n","video 1/1 (frame 87/96) /content/105.mp4: 384x640 4 persons, 8.6ms\n","video 1/1 (frame 88/96) /content/105.mp4: 384x640 4 persons, 8.8ms\n","video 1/1 (frame 89/96) /content/105.mp4: 384x640 4 persons, 7.3ms\n","video 1/1 (frame 90/96) /content/105.mp4: 384x640 4 persons, 9.6ms\n","video 1/1 (frame 91/96) /content/105.mp4: 384x640 4 persons, 8.0ms\n","video 1/1 (frame 92/96) /content/105.mp4: 384x640 5 persons, 11.3ms\n","video 1/1 (frame 93/96) /content/105.mp4: 384x640 4 persons, 9.8ms\n","video 1/1 (frame 94/96) /content/105.mp4: 384x640 4 persons, 9.0ms\n","video 1/1 (frame 95/96) /content/105.mp4: 384x640 4 persons, 6.6ms\n","video 1/1 (frame 96/96) /content/105.mp4: 384x640 4 persons, 8.1ms\n","Speed: 3.4ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"]}]},{"cell_type":"code","source":["for i, result in enumerate(results):\n","    print(f\"Frame {i}: {result.boxes.data}\")  # Print bounding box data\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wz8_6XFNQyuS","executionInfo":{"status":"ok","timestamp":1733996788792,"user_tz":-330,"elapsed":434,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}},"outputId":"6e13b956-d818-47ef-daf9-c8415aec60e5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Frame 0: tensor([[7.8334e+02, 4.9968e+02, 1.1229e+03, 1.0751e+03, 9.2867e-01, 0.0000e+00],\n","        [1.2896e+03, 2.5909e+02, 1.5375e+03, 7.1457e+02, 8.1476e-01, 0.0000e+00],\n","        [1.3392e+03, 3.6991e+02, 1.6773e+03, 9.0189e+02, 8.0074e-01, 0.0000e+00],\n","        [1.5542e+03, 9.2145e+02, 1.8389e+03, 1.0767e+03, 4.3688e-01, 0.0000e+00]], device='cuda:0')\n","Frame 1: tensor([[7.7589e+02, 4.9211e+02, 1.1120e+03, 1.0740e+03, 9.1620e-01, 0.0000e+00],\n","        [1.2854e+03, 2.5945e+02, 1.5369e+03, 7.1353e+02, 8.4649e-01, 0.0000e+00],\n","        [1.3459e+03, 3.7373e+02, 1.6664e+03, 9.0486e+02, 8.3589e-01, 0.0000e+00],\n","        [1.5443e+03, 9.2010e+02, 1.8373e+03, 1.0759e+03, 3.5318e-01, 0.0000e+00]], device='cuda:0')\n","Frame 2: tensor([[7.7555e+02, 4.8578e+02, 1.1005e+03, 1.0754e+03, 9.1403e-01, 0.0000e+00],\n","        [1.2785e+03, 2.5780e+02, 1.5350e+03, 7.0707e+02, 8.6488e-01, 0.0000e+00],\n","        [1.3597e+03, 3.8571e+02, 1.6746e+03, 9.1664e+02, 7.8027e-01, 0.0000e+00],\n","        [1.5355e+03, 9.1910e+02, 1.8346e+03, 1.0760e+03, 3.6727e-01, 0.0000e+00]], device='cuda:0')\n","Frame 3: tensor([[7.7375e+02, 4.8009e+02, 1.0895e+03, 1.0740e+03, 9.0493e-01, 0.0000e+00],\n","        [1.2766e+03, 2.5726e+02, 1.5371e+03, 7.0267e+02, 8.8111e-01, 0.0000e+00],\n","        [1.3632e+03, 3.8122e+02, 1.6714e+03, 9.1756e+02, 7.4546e-01, 0.0000e+00],\n","        [1.5318e+03, 9.1857e+02, 1.8323e+03, 1.0761e+03, 3.9656e-01, 0.0000e+00]], device='cuda:0')\n","Frame 4: tensor([[7.7165e+02, 4.7538e+02, 1.0767e+03, 1.0742e+03, 9.1031e-01, 0.0000e+00],\n","        [1.2695e+03, 2.5828e+02, 1.5371e+03, 7.0411e+02, 8.6280e-01, 0.0000e+00],\n","        [1.3754e+03, 3.9472e+02, 1.6914e+03, 9.1051e+02, 8.0355e-01, 0.0000e+00]], device='cuda:0')\n","Frame 5: tensor([[7.6808e+02, 4.6773e+02, 1.0627e+03, 1.0734e+03, 9.1065e-01, 0.0000e+00],\n","        [1.2661e+03, 2.5701e+02, 1.5418e+03, 6.9503e+02, 8.1723e-01, 0.0000e+00],\n","        [1.4129e+03, 3.9683e+02, 1.6970e+03, 9.1665e+02, 7.1989e-01, 0.0000e+00],\n","        [1.5662e+03, 9.1769e+02, 1.8251e+03, 1.0762e+03, 2.5450e-01, 5.6000e+01]], device='cuda:0')\n","Frame 6: tensor([[7.6244e+02, 4.6316e+02, 1.0498e+03, 1.0733e+03, 9.0852e-01, 0.0000e+00],\n","        [1.2601e+03, 2.5744e+02, 1.5410e+03, 6.9382e+02, 8.5752e-01, 0.0000e+00],\n","        [1.4051e+03, 4.0290e+02, 1.7152e+03, 9.0881e+02, 6.7551e-01, 0.0000e+00],\n","        [7.7096e+02, 5.4906e+02, 8.1984e+02, 5.9425e+02, 3.7080e-01, 6.5000e+01]], device='cuda:0')\n","Frame 7: tensor([[7.5165e+02, 4.5829e+02, 1.0389e+03, 1.0735e+03, 9.0678e-01, 0.0000e+00],\n","        [1.2515e+03, 2.5548e+02, 1.5376e+03, 6.9937e+02, 8.7273e-01, 0.0000e+00]], device='cuda:0')\n","Frame 8: tensor([[7.4491e+02, 4.5373e+02, 1.0290e+03, 1.0754e+03, 9.1729e-01, 0.0000e+00],\n","        [1.2476e+03, 2.4966e+02, 1.5340e+03, 6.9940e+02, 8.6341e-01, 0.0000e+00],\n","        [1.4123e+03, 4.1855e+02, 1.7412e+03, 9.7503e+02, 5.1699e-01, 0.0000e+00]], device='cuda:0')\n","Frame 9: tensor([[7.2075e+02, 4.5141e+02, 1.0282e+03, 1.0694e+03, 9.0720e-01, 0.0000e+00],\n","        [1.2456e+03, 2.5280e+02, 1.5279e+03, 7.0172e+02, 8.4641e-01, 0.0000e+00],\n","        [7.1612e+02, 5.5507e+02, 7.6353e+02, 5.9324e+02, 3.3518e-01, 6.5000e+01],\n","        [1.4247e+03, 4.2813e+02, 1.7544e+03, 9.5278e+02, 2.5311e-01, 1.6000e+01]], device='cuda:0')\n","Frame 10: tensor([[7.0482e+02, 4.4879e+02, 1.0303e+03, 1.0743e+03, 9.0825e-01, 0.0000e+00],\n","        [1.2462e+03, 2.5117e+02, 1.5229e+03, 6.9867e+02, 8.4353e-01, 0.0000e+00],\n","        [1.4271e+03, 4.4154e+02, 1.7651e+03, 1.0382e+03, 7.4651e-01, 0.0000e+00]], device='cuda:0')\n","Frame 11: tensor([[7.0218e+02, 4.4607e+02, 1.0315e+03, 1.0720e+03, 9.0374e-01, 0.0000e+00],\n","        [1.4469e+03, 4.5037e+02, 1.7805e+03, 1.0607e+03, 8.5128e-01, 0.0000e+00],\n","        [1.2457e+03, 2.5015e+02, 1.5178e+03, 6.9836e+02, 8.2820e-01, 0.0000e+00]], device='cuda:0')\n","Frame 12: tensor([[7.0291e+02, 4.4594e+02, 1.0308e+03, 1.0719e+03, 9.0142e-01, 0.0000e+00],\n","        [1.4447e+03, 4.5074e+02, 1.7807e+03, 1.0612e+03, 8.6323e-01, 0.0000e+00],\n","        [1.2438e+03, 2.5130e+02, 1.5145e+03, 7.0109e+02, 8.4044e-01, 0.0000e+00],\n","        [1.5532e+03, 9.7857e+02, 1.7659e+03, 1.0793e+03, 3.0437e-01, 0.0000e+00]], device='cuda:0')\n","Frame 13: tensor([[1.2446e+03, 2.5020e+02, 1.5089e+03, 6.9801e+02, 8.9323e-01, 0.0000e+00],\n","        [7.0173e+02, 4.4467e+02, 1.0227e+03, 1.0745e+03, 8.8978e-01, 0.0000e+00],\n","        [1.4626e+03, 4.6151e+02, 1.8007e+03, 1.0676e+03, 8.4383e-01, 0.0000e+00],\n","        [1.5669e+03, 9.6620e+02, 1.7774e+03, 1.0796e+03, 3.0185e-01, 0.0000e+00],\n","        [9.7831e+02, 7.5137e+02, 1.0246e+03, 7.9578e+02, 3.0098e-01, 6.5000e+01]], device='cuda:0')\n","Frame 14: tensor([[7.0821e+02, 4.4525e+02, 1.0133e+03, 1.0720e+03, 8.7108e-01, 0.0000e+00],\n","        [1.4578e+03, 4.7018e+02, 1.8215e+03, 1.0671e+03, 8.6976e-01, 0.0000e+00],\n","        [1.2452e+03, 2.4871e+02, 1.4984e+03, 7.0187e+02, 8.6584e-01, 0.0000e+00],\n","        [1.5791e+03, 9.5914e+02, 1.7955e+03, 1.0790e+03, 5.0987e-01, 0.0000e+00],\n","        [9.7791e+02, 7.5145e+02, 1.0229e+03, 7.9763e+02, 2.8253e-01, 6.5000e+01]], device='cuda:0')\n","Frame 15: tensor([[7.1507e+02, 4.4285e+02, 1.0109e+03, 1.0761e+03, 8.9183e-01, 0.0000e+00],\n","        [1.2470e+03, 2.4967e+02, 1.4924e+03, 7.0148e+02, 8.6120e-01, 0.0000e+00],\n","        [1.4698e+03, 4.8135e+02, 1.8344e+03, 1.0648e+03, 8.3368e-01, 0.0000e+00],\n","        [1.5836e+03, 9.4710e+02, 1.8013e+03, 1.0790e+03, 6.2834e-01, 0.0000e+00],\n","        [9.8224e+02, 7.6135e+02, 1.0207e+03, 8.0049e+02, 3.4272e-01, 6.5000e+01]], device='cuda:0')\n","Frame 16: tensor([[7.0514e+02, 4.4464e+02, 9.9520e+02, 1.0760e+03, 9.0449e-01, 0.0000e+00],\n","        [1.2461e+03, 2.4611e+02, 1.4832e+03, 7.0253e+02, 8.4300e-01, 0.0000e+00],\n","        [1.4859e+03, 4.9287e+02, 1.8503e+03, 1.0708e+03, 7.7285e-01, 0.0000e+00],\n","        [1.5892e+03, 9.3258e+02, 1.8226e+03, 1.0800e+03, 4.0364e-01, 0.0000e+00],\n","        [9.8310e+02, 7.6622e+02, 1.0244e+03, 8.0475e+02, 3.9303e-01, 6.5000e+01]], device='cuda:0')\n","Frame 17: tensor([[7.0476e+02, 4.4650e+02, 9.9543e+02, 1.0737e+03, 9.1564e-01, 0.0000e+00],\n","        [1.2459e+03, 2.4550e+02, 1.4724e+03, 7.0397e+02, 8.4369e-01, 0.0000e+00],\n","        [1.5943e+03, 9.1578e+02, 1.8094e+03, 1.0800e+03, 6.2754e-01, 0.0000e+00],\n","        [1.4846e+03, 5.0477e+02, 1.8555e+03, 1.0656e+03, 5.8191e-01, 0.0000e+00],\n","        [1.5402e+03, 5.0728e+02, 1.8586e+03, 8.8959e+02, 2.5783e-01, 0.0000e+00]], device='cuda:0')\n","Frame 18: tensor([[7.0481e+02, 4.5090e+02, 9.9461e+02, 1.0718e+03, 9.2019e-01, 0.0000e+00],\n","        [1.2445e+03, 2.4302e+02, 1.4662e+03, 7.0182e+02, 8.6868e-01, 0.0000e+00],\n","        [1.4816e+03, 5.1405e+02, 1.8680e+03, 1.0675e+03, 7.7347e-01, 0.0000e+00],\n","        [1.6204e+03, 9.0134e+02, 1.8137e+03, 1.0799e+03, 7.0302e-01, 0.0000e+00]], device='cuda:0')\n","Frame 19: tensor([[7.0129e+02, 4.5296e+02, 9.8951e+02, 1.0748e+03, 9.1895e-01, 0.0000e+00],\n","        [1.2445e+03, 2.4154e+02, 1.4539e+03, 7.0012e+02, 8.4756e-01, 0.0000e+00],\n","        [1.4891e+03, 5.1979e+02, 1.8934e+03, 1.0705e+03, 6.9269e-01, 0.0000e+00],\n","        [1.6305e+03, 8.8622e+02, 1.8111e+03, 1.0799e+03, 4.9503e-01, 0.0000e+00]], device='cuda:0')\n","Frame 20: tensor([[6.9891e+02, 4.5924e+02, 9.7739e+02, 1.0745e+03, 9.1408e-01, 0.0000e+00],\n","        [1.2466e+03, 2.3982e+02, 1.4480e+03, 6.8506e+02, 8.5212e-01, 0.0000e+00],\n","        [1.4857e+03, 5.2763e+02, 1.8591e+03, 1.0467e+03, 7.2764e-01, 0.0000e+00],\n","        [1.6077e+03, 8.8371e+02, 1.8165e+03, 1.0800e+03, 4.8153e-01, 0.0000e+00],\n","        [1.6808e+03, 8.8519e+02, 1.8116e+03, 1.0799e+03, 3.9893e-01, 0.0000e+00]], device='cuda:0')\n","Frame 21: tensor([[6.9479e+02, 4.6385e+02, 9.7104e+02, 1.0745e+03, 8.9565e-01, 0.0000e+00],\n","        [1.2459e+03, 2.4258e+02, 1.4437e+03, 6.8658e+02, 8.8244e-01, 0.0000e+00],\n","        [1.4787e+03, 5.3534e+02, 1.8660e+03, 1.0632e+03, 7.9654e-01, 0.0000e+00],\n","        [1.6030e+03, 8.8238e+02, 1.8318e+03, 1.0800e+03, 3.7055e-01, 0.0000e+00]], device='cuda:0')\n","Frame 22: tensor([[6.8620e+02, 4.7075e+02, 9.6064e+02, 1.0752e+03, 8.8942e-01, 0.0000e+00],\n","        [1.2477e+03, 2.3770e+02, 1.4367e+03, 6.7872e+02, 8.5834e-01, 0.0000e+00],\n","        [1.4802e+03, 5.4084e+02, 1.8522e+03, 1.0693e+03, 8.0234e-01, 0.0000e+00]], device='cuda:0')\n","Frame 23: tensor([[6.8273e+02, 4.7784e+02, 9.4905e+02, 1.0757e+03, 8.8064e-01, 0.0000e+00],\n","        [1.2463e+03, 2.3222e+02, 1.4308e+03, 6.8091e+02, 8.4925e-01, 0.0000e+00],\n","        [1.4841e+03, 5.4456e+02, 1.8524e+03, 1.0726e+03, 7.4079e-01, 0.0000e+00],\n","        [1.5952e+03, 8.8521e+02, 1.8440e+03, 1.0800e+03, 3.0064e-01, 0.0000e+00]], device='cuda:0')\n","Frame 24: tensor([[6.8035e+02, 4.7725e+02, 9.4234e+02, 1.0768e+03, 8.7355e-01, 0.0000e+00],\n","        [1.2461e+03, 2.2977e+02, 1.4264e+03, 6.7876e+02, 8.5607e-01, 0.0000e+00],\n","        [1.4889e+03, 5.5086e+02, 1.8347e+03, 1.0704e+03, 8.2826e-01, 0.0000e+00],\n","        [1.6960e+03, 9.0340e+02, 1.8501e+03, 1.0800e+03, 3.0074e-01, 0.0000e+00],\n","        [1.6265e+03, 9.0383e+02, 1.8512e+03, 1.0800e+03, 2.8459e-01, 0.0000e+00]], device='cuda:0')\n","Frame 25: tensor([[6.8047e+02, 4.7739e+02, 9.4197e+02, 1.0768e+03, 8.7303e-01, 0.0000e+00],\n","        [1.2463e+03, 2.2977e+02, 1.4263e+03, 6.8007e+02, 8.5885e-01, 0.0000e+00],\n","        [1.4870e+03, 5.5071e+02, 1.8346e+03, 1.0710e+03, 8.2854e-01, 0.0000e+00],\n","        [1.6980e+03, 9.0731e+02, 1.8460e+03, 1.0796e+03, 3.1632e-01, 0.0000e+00],\n","        [1.6252e+03, 9.0404e+02, 1.8512e+03, 1.0800e+03, 2.8847e-01, 0.0000e+00]], device='cuda:0')\n","Frame 26: tensor([[6.7525e+02, 4.7934e+02, 9.2068e+02, 1.0769e+03, 8.9360e-01, 0.0000e+00],\n","        [1.2485e+03, 2.2678e+02, 1.4242e+03, 6.7735e+02, 8.5562e-01, 0.0000e+00],\n","        [1.4811e+03, 5.5717e+02, 1.9108e+03, 1.0751e+03, 8.2163e-01, 0.0000e+00],\n","        [1.6971e+03, 9.0706e+02, 1.8497e+03, 1.0800e+03, 3.4819e-01, 0.0000e+00],\n","        [1.6055e+03, 9.0654e+02, 1.8472e+03, 1.0800e+03, 2.7783e-01, 0.0000e+00]], device='cuda:0')\n","Frame 27: tensor([[6.7418e+02, 4.8114e+02, 9.1424e+02, 1.0756e+03, 8.9871e-01, 0.0000e+00],\n","        [1.2477e+03, 2.2721e+02, 1.4189e+03, 6.8336e+02, 8.7223e-01, 0.0000e+00],\n","        [1.4776e+03, 5.6483e+02, 1.9187e+03, 1.0766e+03, 8.5915e-01, 0.0000e+00]], device='cuda:0')\n","Frame 28: tensor([[6.7142e+02, 4.8252e+02, 9.1248e+02, 1.0760e+03, 8.9279e-01, 0.0000e+00],\n","        [1.2467e+03, 2.2769e+02, 1.4103e+03, 6.4132e+02, 8.8157e-01, 0.0000e+00],\n","        [1.4771e+03, 5.7039e+02, 1.8423e+03, 1.0762e+03, 8.0789e-01, 0.0000e+00],\n","        [1.5884e+03, 9.0120e+02, 1.8291e+03, 1.0800e+03, 3.4646e-01, 0.0000e+00]], device='cuda:0')\n","Frame 29: tensor([[6.6647e+02, 4.8432e+02, 9.0867e+02, 1.0750e+03, 8.9339e-01, 0.0000e+00],\n","        [1.4778e+03, 5.7645e+02, 1.8403e+03, 1.0753e+03, 8.6443e-01, 0.0000e+00],\n","        [1.2237e+03, 2.2599e+02, 1.4058e+03, 6.6272e+02, 8.4883e-01, 0.0000e+00],\n","        [1.5888e+03, 8.8263e+02, 1.8314e+03, 1.0800e+03, 2.7083e-01, 0.0000e+00]], device='cuda:0')\n","Frame 30: tensor([[6.6410e+02, 4.8423e+02, 9.0455e+02, 1.0755e+03, 8.9006e-01, 0.0000e+00],\n","        [1.4790e+03, 5.8563e+02, 1.8465e+03, 1.0768e+03, 8.8475e-01, 0.0000e+00],\n","        [1.1967e+03, 2.2482e+02, 1.3995e+03, 6.8457e+02, 8.7947e-01, 0.0000e+00],\n","        [1.5157e+03, 8.5014e+01, 1.8536e+03, 6.7618e+02, 4.0204e-01, 6.2000e+01]], device='cuda:0')\n","Frame 31: tensor([[1.4770e+03, 5.8709e+02, 1.8604e+03, 1.0755e+03, 8.8675e-01, 0.0000e+00],\n","        [6.5988e+02, 4.8776e+02, 8.9697e+02, 1.0747e+03, 8.8266e-01, 0.0000e+00],\n","        [1.1787e+03, 2.2677e+02, 1.3920e+03, 6.7835e+02, 8.5029e-01, 0.0000e+00],\n","        [1.5206e+03, 8.3563e+01, 1.8541e+03, 6.7467e+02, 3.4255e-01, 6.2000e+01]], device='cuda:0')\n","Frame 32: tensor([[1.4721e+03, 5.8888e+02, 1.8663e+03, 1.0746e+03, 8.9837e-01, 0.0000e+00],\n","        [6.5675e+02, 4.8766e+02, 8.7628e+02, 1.0756e+03, 8.6250e-01, 0.0000e+00],\n","        [1.1808e+03, 2.2676e+02, 1.3839e+03, 6.5439e+02, 7.8894e-01, 0.0000e+00],\n","        [1.6743e+03, 8.9744e+02, 1.8593e+03, 1.0800e+03, 5.3479e-01, 0.0000e+00],\n","        [1.5203e+03, 8.2255e+01, 1.8563e+03, 6.7625e+02, 2.6458e-01, 6.2000e+01]], device='cuda:0')\n","Frame 33: tensor([[1.4794e+03, 5.8874e+02, 1.8736e+03, 1.0747e+03, 8.9099e-01, 0.0000e+00],\n","        [1.1590e+03, 2.2710e+02, 1.3775e+03, 6.7136e+02, 8.7173e-01, 0.0000e+00],\n","        [6.5401e+02, 4.8520e+02, 8.7343e+02, 1.0773e+03, 8.7021e-01, 0.0000e+00],\n","        [1.6785e+03, 9.0436e+02, 1.8688e+03, 1.0800e+03, 2.9386e-01, 0.0000e+00]], device='cuda:0')\n","Frame 34: tensor([[1.4778e+03, 5.8881e+02, 1.8836e+03, 1.0759e+03, 9.0356e-01, 0.0000e+00],\n","        [1.1461e+03, 2.2759e+02, 1.3705e+03, 6.7592e+02, 8.9230e-01, 0.0000e+00],\n","        [6.5003e+02, 4.8601e+02, 8.7395e+02, 1.0761e+03, 8.8150e-01, 0.0000e+00]], device='cuda:0')\n","Frame 35: tensor([[1.4787e+03, 5.8876e+02, 1.8959e+03, 1.0766e+03, 9.0487e-01, 0.0000e+00],\n","        [6.4220e+02, 4.8353e+02, 8.7482e+02, 1.0769e+03, 8.7871e-01, 0.0000e+00],\n","        [1.1443e+03, 2.2972e+02, 1.3604e+03, 6.7020e+02, 8.3381e-01, 0.0000e+00]], device='cuda:0')\n","Frame 36: tensor([[1.4783e+03, 5.8880e+02, 1.8958e+03, 1.0766e+03, 9.0574e-01, 0.0000e+00],\n","        [6.4220e+02, 4.8349e+02, 8.7478e+02, 1.0770e+03, 8.7876e-01, 0.0000e+00],\n","        [1.1449e+03, 2.2975e+02, 1.3603e+03, 6.6986e+02, 8.3488e-01, 0.0000e+00]], device='cuda:0')\n","Frame 37: tensor([[6.3536e+02, 4.8362e+02, 8.7502e+02, 1.0764e+03, 8.8834e-01, 0.0000e+00],\n","        [1.4792e+03, 5.9025e+02, 1.9027e+03, 1.0755e+03, 8.7720e-01, 0.0000e+00],\n","        [1.1341e+03, 2.2881e+02, 1.3503e+03, 6.7582e+02, 8.6129e-01, 0.0000e+00]], device='cuda:0')\n","Frame 38: tensor([[6.2979e+02, 4.7986e+02, 8.7738e+02, 1.0768e+03, 8.9404e-01, 0.0000e+00],\n","        [1.1317e+03, 2.3112e+02, 1.3444e+03, 6.7823e+02, 8.6386e-01, 0.0000e+00],\n","        [1.4744e+03, 5.9007e+02, 1.8921e+03, 1.0725e+03, 8.4733e-01, 0.0000e+00],\n","        [1.6342e+03, 5.9184e+02, 1.8792e+03, 9.5568e+02, 3.7435e-01, 0.0000e+00]], device='cuda:0')\n","Frame 39: tensor([[6.2696e+02, 4.7753e+02, 8.7550e+02, 1.0754e+03, 9.0191e-01, 0.0000e+00],\n","        [1.1303e+03, 2.2996e+02, 1.3366e+03, 6.7868e+02, 8.7958e-01, 0.0000e+00],\n","        [1.4760e+03, 5.9094e+02, 1.8829e+03, 1.0725e+03, 8.7458e-01, 0.0000e+00],\n","        [1.6334e+03, 5.9231e+02, 1.8823e+03, 9.3550e+02, 3.7488e-01, 0.0000e+00],\n","        [1.5100e+03, 8.1809e+01, 1.8588e+03, 6.8060e+02, 3.0157e-01, 6.2000e+01]], device='cuda:0')\n","Frame 40: tensor([[1.4773e+03, 5.9545e+02, 1.8965e+03, 1.0727e+03, 9.0460e-01, 0.0000e+00],\n","        [6.2229e+02, 4.7675e+02, 8.7219e+02, 1.0755e+03, 9.0034e-01, 0.0000e+00],\n","        [1.1300e+03, 2.2645e+02, 1.3261e+03, 6.7926e+02, 8.7843e-01, 0.0000e+00],\n","        [1.5131e+03, 8.0002e+01, 1.8595e+03, 6.8045e+02, 3.7770e-01, 6.2000e+01]], device='cuda:0')\n","Frame 41: tensor([[6.2065e+02, 4.7295e+02, 8.7192e+02, 1.0748e+03, 8.9635e-01, 0.0000e+00],\n","        [1.4782e+03, 5.9749e+02, 1.8973e+03, 1.0740e+03, 8.8781e-01, 0.0000e+00],\n","        [1.1282e+03, 2.2661e+02, 1.3254e+03, 6.7927e+02, 8.6102e-01, 0.0000e+00],\n","        [1.5079e+03, 7.9301e+01, 1.8590e+03, 6.8277e+02, 3.3148e-01, 6.2000e+01]], device='cuda:0')\n","Frame 42: tensor([[6.2142e+02, 4.7074e+02, 8.7181e+02, 1.0750e+03, 9.0641e-01, 0.0000e+00],\n","        [1.1287e+03, 2.2773e+02, 1.3198e+03, 6.7945e+02, 8.4205e-01, 0.0000e+00],\n","        [1.4796e+03, 5.9882e+02, 1.9006e+03, 1.0716e+03, 8.3672e-01, 0.0000e+00],\n","        [1.6414e+03, 5.9835e+02, 1.8999e+03, 1.0438e+03, 6.5818e-01, 0.0000e+00],\n","        [1.5085e+03, 8.0321e+01, 1.8579e+03, 6.8147e+02, 3.1463e-01, 6.2000e+01]], device='cuda:0')\n","Frame 43: tensor([[6.2218e+02, 4.6594e+02, 8.6618e+02, 1.0757e+03, 9.0841e-01, 0.0000e+00],\n","        [1.1298e+03, 2.2620e+02, 1.3130e+03, 6.7879e+02, 8.2446e-01, 0.0000e+00],\n","        [1.4826e+03, 6.0008e+02, 1.8997e+03, 1.0704e+03, 7.2114e-01, 0.0000e+00],\n","        [1.6417e+03, 6.0044e+02, 1.8966e+03, 1.0440e+03, 5.3579e-01, 0.0000e+00],\n","        [1.5091e+03, 7.6077e+01, 1.8581e+03, 6.8144e+02, 3.5181e-01, 6.2000e+01]], device='cuda:0')\n","Frame 44: tensor([[6.2440e+02, 4.6062e+02, 8.5449e+02, 1.0751e+03, 8.8546e-01, 0.0000e+00],\n","        [1.1344e+03, 2.2614e+02, 1.3065e+03, 6.7731e+02, 8.2497e-01, 0.0000e+00],\n","        [1.4848e+03, 5.9921e+02, 1.8972e+03, 1.0687e+03, 8.0623e-01, 0.0000e+00],\n","        [1.6454e+03, 5.9823e+02, 1.8932e+03, 1.0280e+03, 4.4922e-01, 0.0000e+00],\n","        [1.5100e+03, 7.1835e+01, 1.8580e+03, 6.8155e+02, 3.3047e-01, 6.2000e+01]], device='cuda:0')\n","Frame 45: tensor([[6.2813e+02, 4.5281e+02, 8.5549e+02, 1.0744e+03, 8.8485e-01, 0.0000e+00],\n","        [1.1323e+03, 2.2499e+02, 1.3011e+03, 6.7627e+02, 8.4458e-01, 0.0000e+00],\n","        [1.6437e+03, 5.9764e+02, 1.8938e+03, 1.0495e+03, 7.3139e-01, 0.0000e+00],\n","        [1.4797e+03, 5.9613e+02, 1.8947e+03, 1.0619e+03, 7.2521e-01, 0.0000e+00],\n","        [1.5084e+03, 7.0641e+01, 1.8585e+03, 6.8217e+02, 3.4727e-01, 6.2000e+01]], device='cuda:0')\n","Frame 46: tensor([[6.2984e+02, 4.4650e+02, 8.4249e+02, 1.0751e+03, 8.8505e-01, 0.0000e+00],\n","        [1.1253e+03, 2.2457e+02, 1.2949e+03, 6.7934e+02, 8.7062e-01, 0.0000e+00],\n","        [1.6451e+03, 5.9341e+02, 1.8921e+03, 1.0073e+03, 8.4850e-01, 0.0000e+00],\n","        [1.4815e+03, 5.9205e+02, 1.8963e+03, 1.0545e+03, 4.0836e-01, 0.0000e+00],\n","        [1.5125e+03, 7.4270e+01, 1.8597e+03, 6.7884e+02, 3.6418e-01, 6.2000e+01]], device='cuda:0')\n","Frame 47: tensor([[6.2877e+02, 4.3900e+02, 8.3112e+02, 1.0749e+03, 8.9692e-01, 0.0000e+00],\n","        [1.1248e+03, 2.2218e+02, 1.2859e+03, 6.8015e+02, 8.9426e-01, 0.0000e+00],\n","        [1.6396e+03, 5.8815e+02, 1.8910e+03, 1.0062e+03, 7.6729e-01, 0.0000e+00],\n","        [1.5102e+03, 6.8519e+01, 1.8601e+03, 6.7848e+02, 2.6054e-01, 6.2000e+01]], device='cuda:0')\n","Frame 48: tensor([[6.2212e+02, 4.3544e+02, 8.2689e+02, 1.0758e+03, 8.9675e-01, 0.0000e+00],\n","        [1.1163e+03, 2.2211e+02, 1.2824e+03, 6.8150e+02, 8.9533e-01, 0.0000e+00],\n","        [1.5600e+03, 5.7545e+02, 1.8856e+03, 1.0689e+03, 7.2602e-01, 0.0000e+00],\n","        [1.5120e+03, 7.5305e+01, 1.8593e+03, 6.7730e+02, 3.1420e-01, 6.2000e+01]], device='cuda:0')\n","Frame 49: tensor([[6.2218e+02, 4.3556e+02, 8.2693e+02, 1.0758e+03, 8.9743e-01, 0.0000e+00],\n","        [1.1168e+03, 2.2224e+02, 1.2826e+03, 6.8141e+02, 8.9439e-01, 0.0000e+00],\n","        [1.5571e+03, 5.7563e+02, 1.8857e+03, 1.0689e+03, 7.2061e-01, 0.0000e+00],\n","        [1.5117e+03, 7.5251e+01, 1.8593e+03, 6.7793e+02, 3.3764e-01, 6.2000e+01]], device='cuda:0')\n","Frame 50: tensor([[6.2112e+02, 4.3039e+02, 8.2896e+02, 1.0761e+03, 9.0324e-01, 0.0000e+00],\n","        [1.1074e+03, 2.1876e+02, 1.2789e+03, 6.8173e+02, 9.0226e-01, 0.0000e+00],\n","        [1.5496e+03, 5.6804e+02, 1.8809e+03, 1.0694e+03, 7.3555e-01, 0.0000e+00],\n","        [1.5115e+03, 6.2124e+01, 1.8623e+03, 6.6936e+02, 3.1658e-01, 6.2000e+01]], device='cuda:0')\n","Frame 51: tensor([[6.1852e+02, 4.1867e+02, 8.3297e+02, 1.0764e+03, 8.9894e-01, 0.0000e+00],\n","        [1.0978e+03, 2.2035e+02, 1.2737e+03, 6.8233e+02, 8.7043e-01, 0.0000e+00],\n","        [1.5208e+03, 5.6132e+02, 1.8770e+03, 1.0709e+03, 7.6748e-01, 0.0000e+00],\n","        [1.5099e+03, 5.7945e+01, 1.8650e+03, 6.6837e+02, 2.5145e-01, 6.2000e+01]], device='cuda:0')\n","Frame 52: tensor([[6.1555e+02, 4.1014e+02, 8.3180e+02, 1.0777e+03, 8.9709e-01, 0.0000e+00],\n","        [1.0911e+03, 2.2330e+02, 1.2700e+03, 6.7678e+02, 8.8682e-01, 0.0000e+00],\n","        [1.4995e+03, 5.5319e+02, 1.8684e+03, 1.0689e+03, 7.3544e-01, 0.0000e+00]], device='cuda:0')\n","Frame 53: tensor([[6.1323e+02, 4.0248e+02, 8.3288e+02, 1.0779e+03, 8.7223e-01, 0.0000e+00],\n","        [1.0807e+03, 2.2363e+02, 1.2640e+03, 6.7751e+02, 8.6715e-01, 0.0000e+00],\n","        [1.4611e+03, 5.4630e+02, 1.8614e+03, 1.0698e+03, 6.7724e-01, 0.0000e+00],\n","        [1.5109e+03, 6.7008e+01, 1.8626e+03, 6.6362e+02, 3.2118e-01, 6.2000e+01]], device='cuda:0')\n","Frame 54: tensor([[1.0406e+03, 2.2272e+02, 1.2580e+03, 6.8039e+02, 8.6413e-01, 0.0000e+00],\n","        [6.0816e+02, 3.9107e+02, 8.2793e+02, 1.0769e+03, 8.4036e-01, 0.0000e+00],\n","        [1.4580e+03, 5.3971e+02, 1.8532e+03, 1.0679e+03, 3.2137e-01, 0.0000e+00],\n","        [7.4711e+02, 6.4663e+02, 7.9417e+02, 6.9485e+02, 2.6817e-01, 6.5000e+01]], device='cuda:0')\n","Frame 55: tensor([[1.0232e+03, 2.2327e+02, 1.2523e+03, 6.7947e+02, 8.9452e-01, 0.0000e+00],\n","        [6.0365e+02, 3.7813e+02, 8.2285e+02, 1.0672e+03, 8.5708e-01, 0.0000e+00],\n","        [1.4615e+03, 5.3295e+02, 1.8446e+03, 1.0676e+03, 3.7111e-01, 1.5000e+01],\n","        [1.5172e+03, 6.6422e+01, 1.8614e+03, 6.4731e+02, 3.1157e-01, 6.2000e+01]], device='cuda:0')\n","Frame 56: tensor([[1.0040e+03, 2.2452e+02, 1.2447e+03, 6.7940e+02, 9.1346e-01, 0.0000e+00],\n","        [5.9905e+02, 3.6751e+02, 8.1938e+02, 1.0376e+03, 8.6501e-01, 0.0000e+00],\n","        [1.4615e+03, 5.2256e+02, 1.8312e+03, 1.0674e+03, 7.3136e-01, 1.6000e+01],\n","        [7.4701e+02, 6.2228e+02, 7.8428e+02, 6.5757e+02, 2.5624e-01, 6.5000e+01]], device='cuda:0')\n","Frame 57: tensor([[9.9875e+02, 2.2161e+02, 1.2372e+03, 6.7513e+02, 9.1773e-01, 0.0000e+00],\n","        [5.9466e+02, 3.5488e+02, 8.2076e+02, 1.0145e+03, 8.7951e-01, 0.0000e+00],\n","        [1.4611e+03, 5.1631e+02, 1.8227e+03, 1.0621e+03, 7.7084e-01, 1.6000e+01]], device='cuda:0')\n","Frame 58: tensor([[9.9288e+02, 2.2278e+02, 1.2315e+03, 6.7839e+02, 9.1962e-01, 0.0000e+00],\n","        [5.8651e+02, 3.4082e+02, 8.2119e+02, 9.9147e+02, 9.0381e-01, 0.0000e+00],\n","        [1.4602e+03, 5.1102e+02, 1.8105e+03, 1.0582e+03, 6.2013e-01, 1.6000e+01],\n","        [1.4639e+03, 5.0909e+02, 1.8119e+03, 1.0616e+03, 3.1697e-01, 1.5000e+01]], device='cuda:0')\n","Frame 59: tensor([[9.9176e+02, 2.2555e+02, 1.2240e+03, 6.8055e+02, 9.1490e-01, 0.0000e+00],\n","        [5.8775e+02, 3.3070e+02, 8.2198e+02, 9.5032e+02, 8.8938e-01, 0.0000e+00],\n","        [1.4603e+03, 5.0525e+02, 1.8016e+03, 1.0576e+03, 5.7940e-01, 1.6000e+01]], device='cuda:0')\n","Frame 60: tensor([[5.7957e+02, 3.2058e+02, 8.2224e+02, 9.2660e+02, 9.1281e-01, 0.0000e+00],\n","        [9.9293e+02, 2.2253e+02, 1.2231e+03, 6.8061e+02, 9.0480e-01, 0.0000e+00],\n","        [1.4648e+03, 4.9611e+02, 1.7944e+03, 1.0646e+03, 3.4742e-01, 1.6000e+01]], device='cuda:0')\n","Frame 61: tensor([[5.7994e+02, 3.2051e+02, 8.2213e+02, 9.2640e+02, 9.1063e-01, 0.0000e+00],\n","        [9.9238e+02, 2.2244e+02, 1.2233e+03, 6.8080e+02, 9.0284e-01, 0.0000e+00],\n","        [1.4613e+03, 4.9511e+02, 1.7930e+03, 1.0640e+03, 3.7162e-01, 1.6000e+01]], device='cuda:0')\n","Frame 62: tensor([[9.9142e+02, 2.2088e+02, 1.2179e+03, 6.7880e+02, 9.0023e-01, 0.0000e+00],\n","        [5.7416e+02, 3.1375e+02, 8.2220e+02, 9.1846e+02, 8.9276e-01, 0.0000e+00],\n","        [1.4940e+03, 4.8466e+02, 1.7861e+03, 1.0484e+03, 4.7756e-01, 0.0000e+00]], device='cuda:0')\n","Frame 63: tensor([[5.7561e+02, 3.0212e+02, 8.2397e+02, 9.1595e+02, 8.9920e-01, 0.0000e+00],\n","        [9.8977e+02, 2.2080e+02, 1.2101e+03, 6.7865e+02, 8.9902e-01, 0.0000e+00],\n","        [1.4617e+03, 4.7882e+02, 1.7743e+03, 1.0185e+03, 5.8900e-01, 1.6000e+01]], device='cuda:0')\n","Frame 64: tensor([[5.7807e+02, 2.9802e+02, 8.2065e+02, 9.1476e+02, 8.8963e-01, 0.0000e+00],\n","        [9.8966e+02, 2.1847e+02, 1.2037e+03, 6.8055e+02, 8.8342e-01, 0.0000e+00],\n","        [1.4529e+03, 4.6848e+02, 1.7658e+03, 1.0078e+03, 4.4705e-01, 0.0000e+00],\n","        [1.4523e+03, 4.6863e+02, 1.7660e+03, 1.0077e+03, 3.6269e-01, 1.6000e+01]], device='cuda:0')\n","Frame 65: tensor([[9.9010e+02, 2.1507e+02, 1.1990e+03, 6.8271e+02, 8.8409e-01, 0.0000e+00],\n","        [5.8151e+02, 2.9293e+02, 8.0804e+02, 9.1163e+02, 8.7486e-01, 0.0000e+00],\n","        [1.4346e+03, 4.5947e+02, 1.7572e+03, 9.8534e+02, 5.9289e-01, 0.0000e+00]], device='cuda:0')\n","Frame 66: tensor([[9.8954e+02, 2.1811e+02, 1.1922e+03, 6.8356e+02, 8.8351e-01, 0.0000e+00],\n","        [5.8603e+02, 2.9127e+02, 8.1241e+02, 9.1008e+02, 8.7056e-01, 0.0000e+00],\n","        [1.4050e+03, 4.5647e+02, 1.7501e+03, 9.6579e+02, 6.3850e-01, 0.0000e+00]], device='cuda:0')\n","Frame 67: tensor([[9.9329e+02, 2.1627e+02, 1.1859e+03, 6.8404e+02, 8.8708e-01, 0.0000e+00],\n","        [5.8852e+02, 2.8929e+02, 8.0318e+02, 9.0732e+02, 8.6781e-01, 0.0000e+00],\n","        [1.3798e+03, 4.5023e+02, 1.7362e+03, 9.5867e+02, 6.0963e-01, 0.0000e+00],\n","        [1.8109e+03, 1.0012e+03, 1.9195e+03, 1.0800e+03, 2.5078e-01, 0.0000e+00]], device='cuda:0')\n","Frame 68: tensor([[9.9009e+02, 2.1757e+02, 1.1791e+03, 6.8260e+02, 8.9108e-01, 0.0000e+00],\n","        [1.3543e+03, 4.4684e+02, 1.7248e+03, 9.5679e+02, 8.7698e-01, 0.0000e+00],\n","        [5.9606e+02, 2.8440e+02, 7.9949e+02, 9.0361e+02, 8.5054e-01, 0.0000e+00],\n","        [1.7918e+03, 1.0012e+03, 1.9195e+03, 1.0800e+03, 5.4352e-01, 0.0000e+00]], device='cuda:0')\n","Frame 69: tensor([[9.8840e+02, 2.1733e+02, 1.1727e+03, 6.8258e+02, 8.8281e-01, 0.0000e+00],\n","        [5.9607e+02, 2.7967e+02, 8.0009e+02, 8.9762e+02, 8.7417e-01, 0.0000e+00],\n","        [1.3366e+03, 4.4055e+02, 1.7103e+03, 9.5943e+02, 6.1621e-01, 0.0000e+00],\n","        [1.7737e+03, 9.9668e+02, 1.9192e+03, 1.0800e+03, 4.9747e-01, 0.0000e+00]], device='cuda:0')\n","Frame 70: tensor([[5.9861e+02, 2.7261e+02, 8.0232e+02, 8.8680e+02, 8.8587e-01, 0.0000e+00],\n","        [9.8684e+02, 2.1387e+02, 1.1638e+03, 6.8204e+02, 8.5570e-01, 0.0000e+00],\n","        [1.7621e+03, 9.9556e+02, 1.9195e+03, 1.0800e+03, 4.4039e-01, 0.0000e+00],\n","        [1.3321e+03, 4.2754e+02, 1.6957e+03, 9.5497e+02, 3.2866e-01, 0.0000e+00]], device='cuda:0')\n","Frame 71: tensor([[5.9744e+02, 2.6559e+02, 8.0730e+02, 8.7547e+02, 8.9811e-01, 0.0000e+00],\n","        [9.8298e+02, 2.1231e+02, 1.1611e+03, 6.7741e+02, 8.6511e-01, 0.0000e+00],\n","        [1.3216e+03, 4.2892e+02, 1.6899e+03, 9.5510e+02, 6.2492e-01, 0.0000e+00],\n","        [1.7462e+03, 9.9450e+02, 1.9193e+03, 1.0800e+03, 4.2166e-01, 0.0000e+00]], device='cuda:0')\n","Frame 72: tensor([[5.9853e+02, 2.5330e+02, 8.1053e+02, 8.6703e+02, 8.9370e-01, 0.0000e+00],\n","        [9.8201e+02, 2.0924e+02, 1.1586e+03, 6.6944e+02, 8.8919e-01, 0.0000e+00],\n","        [1.3179e+03, 4.2409e+02, 1.6817e+03, 9.5091e+02, 5.6165e-01, 0.0000e+00],\n","        [1.7316e+03, 9.9576e+02, 1.9187e+03, 1.0789e+03, 3.7521e-01, 0.0000e+00]], device='cuda:0')\n","Frame 73: tensor([[5.9859e+02, 2.5303e+02, 8.1041e+02, 8.6711e+02, 8.9425e-01, 0.0000e+00],\n","        [9.8198e+02, 2.0926e+02, 1.1580e+03, 6.6939e+02, 8.9049e-01, 0.0000e+00],\n","        [1.3174e+03, 4.2436e+02, 1.6815e+03, 9.5081e+02, 5.1710e-01, 0.0000e+00],\n","        [1.7320e+03, 9.9517e+02, 1.9187e+03, 1.0789e+03, 3.5794e-01, 0.0000e+00]], device='cuda:0')\n","Frame 74: tensor([[9.7942e+02, 2.0690e+02, 1.1532e+03, 6.6402e+02, 8.8477e-01, 0.0000e+00],\n","        [6.0005e+02, 2.4227e+02, 8.1277e+02, 8.3860e+02, 8.7937e-01, 0.0000e+00],\n","        [1.3111e+03, 4.2255e+02, 1.6769e+03, 9.4923e+02, 8.4042e-01, 0.0000e+00],\n","        [1.7186e+03, 9.9970e+02, 1.9104e+03, 1.0800e+03, 4.7368e-01, 0.0000e+00]], device='cuda:0')\n","Frame 75: tensor([[1.3088e+03, 4.1480e+02, 1.6748e+03, 9.4328e+02, 8.7695e-01, 0.0000e+00],\n","        [9.7928e+02, 2.0816e+02, 1.1517e+03, 6.6445e+02, 8.6132e-01, 0.0000e+00],\n","        [5.9216e+02, 2.3562e+02, 8.1400e+02, 8.0372e+02, 8.4979e-01, 0.0000e+00],\n","        [1.7062e+03, 1.0059e+03, 1.8917e+03, 1.0800e+03, 4.2465e-01, 0.0000e+00]], device='cuda:0')\n","Frame 76: tensor([[9.8027e+02, 2.0629e+02, 1.1437e+03, 6.6125e+02, 8.7657e-01, 0.0000e+00],\n","        [5.6509e+02, 2.2651e+02, 8.1351e+02, 7.6415e+02, 8.6001e-01, 0.0000e+00],\n","        [1.3089e+03, 4.1278e+02, 1.6617e+03, 9.3611e+02, 7.9334e-01, 0.0000e+00]], device='cuda:0')\n","Frame 77: tensor([[9.8091e+02, 2.0510e+02, 1.1415e+03, 6.6052e+02, 8.9237e-01, 0.0000e+00],\n","        [5.8558e+02, 2.2136e+02, 8.1066e+02, 7.4645e+02, 8.8065e-01, 0.0000e+00],\n","        [1.3100e+03, 4.0989e+02, 1.6574e+03, 9.2125e+02, 7.5559e-01, 0.0000e+00]], device='cuda:0')\n","Frame 78: tensor([[5.8857e+02, 2.1603e+02, 8.0856e+02, 7.5052e+02, 8.8743e-01, 0.0000e+00],\n","        [9.8266e+02, 2.0377e+02, 1.1377e+03, 6.6139e+02, 8.8213e-01, 0.0000e+00],\n","        [1.3093e+03, 4.0186e+02, 1.6410e+03, 8.8961e+02, 6.8622e-01, 0.0000e+00]], device='cuda:0')\n","Frame 79: tensor([[5.8779e+02, 2.0783e+02, 8.0451e+02, 7.4074e+02, 8.9633e-01, 0.0000e+00],\n","        [9.7918e+02, 2.0248e+02, 1.1368e+03, 6.6435e+02, 8.6922e-01, 0.0000e+00],\n","        [1.3110e+03, 3.9828e+02, 1.6303e+03, 8.7425e+02, 6.5151e-01, 0.0000e+00]], device='cuda:0')\n","Frame 80: tensor([[5.8846e+02, 2.0488e+02, 8.0360e+02, 7.4646e+02, 8.8598e-01, 0.0000e+00],\n","        [9.7605e+02, 2.0200e+02, 1.1390e+03, 6.6468e+02, 8.6989e-01, 0.0000e+00],\n","        [1.3111e+03, 3.8918e+02, 1.6248e+03, 8.6076e+02, 6.6124e-01, 0.0000e+00]], device='cuda:0')\n","Frame 81: tensor([[5.8839e+02, 2.0290e+02, 8.0060e+02, 7.4031e+02, 8.7954e-01, 0.0000e+00],\n","        [9.7001e+02, 1.9823e+02, 1.1412e+03, 6.6382e+02, 8.7319e-01, 0.0000e+00],\n","        [1.3087e+03, 3.8007e+02, 1.6144e+03, 8.6053e+02, 7.9043e-01, 0.0000e+00]], device='cuda:0')\n","Frame 82: tensor([[5.8507e+02, 2.0188e+02, 7.9957e+02, 7.3447e+02, 8.7143e-01, 0.0000e+00],\n","        [9.6844e+02, 1.9752e+02, 1.1463e+03, 6.6365e+02, 8.6929e-01, 0.0000e+00],\n","        [1.3075e+03, 3.7222e+02, 1.6076e+03, 8.6010e+02, 8.2967e-01, 0.0000e+00]], device='cuda:0')\n","Frame 83: tensor([[1.3077e+03, 3.6355e+02, 1.6054e+03, 8.6066e+02, 8.9202e-01, 0.0000e+00],\n","        [9.6813e+02, 2.0119e+02, 1.1507e+03, 6.6584e+02, 8.9007e-01, 0.0000e+00],\n","        [5.8131e+02, 2.0031e+02, 7.9594e+02, 7.2476e+02, 8.5180e-01, 0.0000e+00],\n","        [1.6356e+03, 9.4100e+02, 1.7839e+03, 1.0786e+03, 2.6607e-01, 0.0000e+00]], device='cuda:0')\n","Frame 84: tensor([[9.6843e+02, 2.0113e+02, 1.1549e+03, 6.6223e+02, 8.7611e-01, 0.0000e+00],\n","        [1.3078e+03, 3.5697e+02, 1.6021e+03, 8.6223e+02, 8.6979e-01, 0.0000e+00],\n","        [5.6636e+02, 1.9940e+02, 7.9498e+02, 7.2006e+02, 8.5331e-01, 0.0000e+00]], device='cuda:0')\n","Frame 85: tensor([[9.6821e+02, 2.0096e+02, 1.1550e+03, 6.6256e+02, 8.7970e-01, 0.0000e+00],\n","        [5.6628e+02, 1.9940e+02, 7.9501e+02, 7.1990e+02, 8.5661e-01, 0.0000e+00],\n","        [1.3080e+03, 3.5702e+02, 1.6019e+03, 8.6220e+02, 8.5042e-01, 0.0000e+00]], device='cuda:0')\n","Frame 86: tensor([[9.7333e+02, 1.9876e+02, 1.1589e+03, 6.5772e+02, 8.7967e-01, 0.0000e+00],\n","        [1.3108e+03, 3.5192e+02, 1.5969e+03, 8.6511e+02, 8.7194e-01, 0.0000e+00],\n","        [5.6677e+02, 1.9665e+02, 7.9319e+02, 7.1652e+02, 8.3941e-01, 0.0000e+00],\n","        [1.5806e+03, 9.4761e+02, 1.7663e+03, 1.0790e+03, 3.2559e-01, 0.0000e+00]], device='cuda:0')\n","Frame 87: tensor([[9.7690e+02, 1.9654e+02, 1.1594e+03, 6.4573e+02, 8.8272e-01, 0.0000e+00],\n","        [5.8377e+02, 1.9609e+02, 7.9224e+02, 7.1079e+02, 8.4309e-01, 0.0000e+00],\n","        [1.3109e+03, 3.4805e+02, 1.5901e+03, 8.6484e+02, 8.2248e-01, 0.0000e+00],\n","        [1.5666e+03, 9.3837e+02, 1.7529e+03, 1.0800e+03, 2.7024e-01, 0.0000e+00]], device='cuda:0')\n","Frame 88: tensor([[9.7919e+02, 1.8981e+02, 1.1584e+03, 6.2630e+02, 8.9236e-01, 0.0000e+00],\n","        [1.2999e+03, 3.4201e+02, 1.5799e+03, 8.6230e+02, 8.8414e-01, 0.0000e+00],\n","        [5.9899e+02, 1.9584e+02, 7.8943e+02, 6.9667e+02, 8.6025e-01, 0.0000e+00],\n","        [1.5422e+03, 9.2068e+02, 1.7361e+03, 1.0793e+03, 5.0752e-01, 0.0000e+00]], device='cuda:0')\n","Frame 89: tensor([[5.9274e+02, 1.8980e+02, 7.8737e+02, 6.7675e+02, 9.0344e-01, 0.0000e+00],\n","        [1.2937e+03, 3.4076e+02, 1.5733e+03, 8.6318e+02, 8.7168e-01, 0.0000e+00],\n","        [9.7359e+02, 1.8566e+02, 1.1595e+03, 6.2786e+02, 8.6632e-01, 0.0000e+00],\n","        [1.5154e+03, 8.9669e+02, 1.7256e+03, 1.0799e+03, 6.6296e-01, 0.0000e+00]], device='cuda:0')\n","Frame 90: tensor([[5.9254e+02, 1.8259e+02, 7.8648e+02, 6.5113e+02, 8.8066e-01, 0.0000e+00],\n","        [9.7338e+02, 1.8058e+02, 1.1592e+03, 6.1170e+02, 8.7247e-01, 0.0000e+00],\n","        [1.2918e+03, 3.3665e+02, 1.5671e+03, 8.5931e+02, 8.3193e-01, 0.0000e+00],\n","        [1.5026e+03, 8.7827e+02, 1.7109e+03, 1.0792e+03, 7.0794e-01, 0.0000e+00]], device='cuda:0')\n","Frame 91: tensor([[5.9957e+02, 1.7690e+02, 7.8821e+02, 6.3360e+02, 8.9243e-01, 0.0000e+00],\n","        [9.6895e+02, 1.7754e+02, 1.1553e+03, 6.0411e+02, 8.7882e-01, 0.0000e+00],\n","        [1.2899e+03, 3.3257e+02, 1.5686e+03, 8.5914e+02, 8.5144e-01, 0.0000e+00],\n","        [1.4015e+03, 6.4693e+02, 1.6888e+03, 1.0704e+03, 3.0987e-01, 0.0000e+00],\n","        [1.4879e+03, 8.4881e+02, 1.6849e+03, 1.0768e+03, 2.5081e-01, 0.0000e+00]], device='cuda:0')\n","Frame 92: tensor([[6.0738e+02, 1.7204e+02, 7.9660e+02, 6.2926e+02, 9.1564e-01, 0.0000e+00],\n","        [9.6665e+02, 1.7742e+02, 1.1572e+03, 5.9811e+02, 9.0140e-01, 0.0000e+00],\n","        [1.2925e+03, 3.2676e+02, 1.5610e+03, 8.5740e+02, 7.4419e-01, 0.0000e+00],\n","        [1.3903e+03, 6.1880e+02, 1.6729e+03, 1.0619e+03, 2.9666e-01, 0.0000e+00]], device='cuda:0')\n","Frame 93: tensor([[6.1288e+02, 1.6765e+02, 7.9796e+02, 6.2722e+02, 9.1829e-01, 0.0000e+00],\n","        [1.2949e+03, 3.2545e+02, 1.5525e+03, 8.5562e+02, 8.8947e-01, 0.0000e+00],\n","        [9.6217e+02, 1.7519e+02, 1.1544e+03, 5.9895e+02, 8.6996e-01, 0.0000e+00],\n","        [1.3005e+03, 6.2586e+02, 1.6621e+03, 1.0724e+03, 2.5865e-01, 0.0000e+00]], device='cuda:0')\n","Frame 94: tensor([[6.0274e+02, 1.6201e+02, 7.9938e+02, 6.2644e+02, 9.0915e-01, 0.0000e+00],\n","        [1.2939e+03, 3.1878e+02, 1.5463e+03, 8.5210e+02, 8.9917e-01, 0.0000e+00],\n","        [9.5846e+02, 1.7263e+02, 1.1528e+03, 5.9842e+02, 8.7153e-01, 0.0000e+00],\n","        [1.2630e+03, 7.6750e+02, 1.6622e+03, 1.0722e+03, 3.3884e-01, 0.0000e+00]], device='cuda:0')\n","Frame 95: tensor([[5.9108e+02, 1.6165e+02, 8.0075e+02, 6.2248e+02, 9.0769e-01, 0.0000e+00],\n","        [1.2928e+03, 3.1024e+02, 1.5444e+03, 8.4706e+02, 8.9253e-01, 0.0000e+00],\n","        [9.5154e+02, 1.6773e+02, 1.1487e+03, 5.9845e+02, 8.5177e-01, 0.0000e+00],\n","        [1.2790e+03, 7.4210e+02, 1.6627e+03, 1.0722e+03, 7.0251e-01, 0.0000e+00]], device='cuda:0')\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Example: Simple rule-based approach\n","for i, result in enumerate(results):\n","    detected_classes = [yolo.names[int(cls)] for cls in result.boxes.cls]\n","    if 'person' in detected_classes and 'knife' in detected_classes:\n","        print(f\"Potential violence detected in Frame {i}\")\n"],"metadata":{"id":"kybPYVTEQ3R2","executionInfo":{"status":"ok","timestamp":1733996810454,"user_tz":-330,"elapsed":364,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import os\n","\n","# Directory where the detection frames are saved\n","save_dir = 'detections/'\n","\n","# Get the list of saved frames, sorted to maintain the correct order\n","frame_files = sorted([os.path.join(save_dir, f) for f in os.listdir(save_dir) if f.endswith('.jpg')])\n","\n","# Check if the list of frames is not empty\n","if len(frame_files) == 0:\n","    print(\"No frames found in the detections directory. Please check the path and ensure frames are saved.\")\n","else:\n","    # Get the frame size from the first frame\n","    sample_frame = cv2.imread(frame_files[0])\n","    frame_size = sample_frame.shape[1::-1]  # (width, height)\n","\n","    # Initialize the video writer\n","    output_video = 'output_results.mp4'\n","    video_writer = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*'mp4v'), 30, frame_size)\n","\n","    # Iterate over frames and write them to the video\n","    for frame_file in frame_files:\n","        frame = cv2.imread(frame_file)\n","        video_writer.write(frame)  #\n"],"metadata":{"id":"vw43q5N2Q3Y_","executionInfo":{"status":"ok","timestamp":1733997018321,"user_tz":-330,"elapsed":3687,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from IPython.display import HTML\n","from base64 import b64encode\n","\n","def display_video(video_path):\n","    with open(video_path, \"rb\") as f:\n","        video = f.read()\n","    video_encoded = b64encode(video).decode(\"ascii\")\n","    return HTML(f\"\"\"\n","    <video width=\"600\" controls>\n","        <source src=\"data:video/mp4;base64,{video_encoded}\" type=\"video/mp4\">\n","    </video>\n","    \"\"\")\n","\n","# Display the video\n","display_video(output_video)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321,"output_embedded_package_id":"1SFMSqsNFzWl99rsQWOAcn2iAq9rWBcOW"},"id":"m7qjxiSuRuuR","executionInfo":{"status":"ok","timestamp":1733997045456,"user_tz":-330,"elapsed":11428,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}},"outputId":"8d08881c-83c4-4665-ddc2-6d192824571c"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import cv2\n","import os\n","from ultralytics import YOLO\n","\n","# Load the YOLO model\n","yolo = YOLO('yolov8n.pt')  # You can choose a model that fits your needs\n","\n","# Directory where the detection frames are saved\n","save_dir = 'detections/'\n","\n","# Get the list of saved frames, sorted to maintain the correct order\n","frame_files = sorted([os.path.join(save_dir, f) for f in os.listdir(save_dir) if f.endswith('.jpg')])\n","\n","# Check if the list of frames is not empty\n","if len(frame_files) == 0:\n","    print(\"No frames found in the detections directory. Please check the path and ensure frames are saved.\")\n","else:\n","    # Get the frame size from the first frame\n","    sample_frame = cv2.imread(frame_files[0])\n","    frame_size = sample_frame.shape[1::-1]  # (width, height)\n","\n","    # Initialize the video writer\n","    output_video = 'output_results.mp4'\n","    video_writer = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*'mp4v'), 30, frame_size)\n","\n","    # Iterate over frames and write them to the video\n","    for frame_file in frame_files:\n","        frame = cv2.imread(frame_file)\n","\n","        # Perform object detection on the current frame\n","        results = yolo(frame)\n","\n","        # Extract bounding box data and labels for detected objects\n","        detected_violence = False  # Default to No Violence Detected\n","        for result in results:\n","            for box in result.boxes:\n","                # Here, you can define which objects/labels indicate violence\n","                label = box.cls  # Get the class label for each detected object\n","                if label == 'weapon' or label == 'aggressive behavior':  # Example for detecting violence\n","                    detected_violence = True\n","                    break\n","\n","        # Add the detection text to the frame\n","        detection_text = \"Violence Detected\" if detected_violence else \"No Violence Detected\"\n","        font = cv2.FONT_HERSHEY_SIMPLEX\n","        cv2.putText(frame, detection_text, (50, 50), font, 1, (0, 0, 255), 2, cv2.LINE_AA)  # Red text\n","\n","        # Write the frame to the video\n","        video_writer.write(frame)\n","\n","    video_writer.release()  # Save the video\n","    print(f\"Video saved as {output_video}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"he8BgcaqPsZ8","executionInfo":{"status":"ok","timestamp":1733997285360,"user_tz":-330,"elapsed":11055,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}},"outputId":"ae24a7ba-a042-4d87-c01f-3795e5a615d1"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 2 persons, 17.3ms\n","Speed: 4.3ms preprocess, 17.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 6.5ms\n","Speed: 3.0ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 6.8ms\n","Speed: 3.5ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 handbag, 15.8ms\n","Speed: 3.4ms preprocess, 15.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 7.1ms\n","Speed: 3.4ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 handbag, 7.2ms\n","Speed: 3.3ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 7.3ms\n","Speed: 3.4ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 7.2ms\n","Speed: 3.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 6.8ms\n","Speed: 3.3ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 7.0ms\n","Speed: 3.6ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 7.3ms\n","Speed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 11.0ms\n","Speed: 3.9ms preprocess, 11.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.8ms\n","Speed: 4.7ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.4ms\n","Speed: 3.6ms preprocess, 12.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.0ms\n","Speed: 3.8ms preprocess, 13.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.7ms\n","Speed: 3.3ms preprocess, 11.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.8ms\n","Speed: 3.0ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.6ms\n","Speed: 4.8ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 7.1ms\n","Speed: 2.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 9.6ms\n","Speed: 3.0ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 6.8ms\n","Speed: 3.3ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 7.0ms\n","Speed: 4.0ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 7.1ms\n","Speed: 2.9ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 horse, 6.8ms\n","Speed: 2.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 7.1ms\n","Speed: 3.4ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.2ms\n","Speed: 3.0ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 7.2ms\n","Speed: 2.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 7.1ms\n","Speed: 3.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 8.4ms\n","Speed: 3.5ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.1ms\n","Speed: 4.0ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 13.9ms\n","Speed: 3.0ms preprocess, 13.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 10.4ms\n","Speed: 3.2ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 18.3ms\n","Speed: 4.3ms preprocess, 18.3ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 teddy bear, 11.7ms\n","Speed: 3.5ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 7.0ms\n","Speed: 3.9ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 tv, 7.0ms\n","Speed: 4.5ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 tv, 7.4ms\n","Speed: 3.4ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 tv, 10.9ms\n","Speed: 3.5ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 7.0ms\n","Speed: 2.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 8.2ms\n","Speed: 2.8ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 tv, 6.9ms\n","Speed: 2.8ms preprocess, 6.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 dog, 1 tv, 6.9ms\n","Speed: 2.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 tv, 6.6ms\n","Speed: 3.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 7.9ms\n","Speed: 3.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 6.9ms\n","Speed: 3.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.2ms\n","Speed: 2.7ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 7.3ms\n","Speed: 3.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 6.8ms\n","Speed: 3.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 9.9ms\n","Speed: 3.3ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 10.1ms\n","Speed: 3.4ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 cat, 9.5ms\n","Speed: 2.7ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 dog, 9.9ms\n","Speed: 4.3ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 dog, 11.7ms\n","Speed: 2.9ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 dog, 11.5ms\n","Speed: 3.2ms preprocess, 11.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 cat, 11.5ms\n","Speed: 3.1ms preprocess, 11.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 15.9ms\n","Speed: 3.1ms preprocess, 15.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 14.1ms\n","Speed: 3.1ms preprocess, 14.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.1ms\n","Speed: 3.3ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.3ms\n","Speed: 3.1ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 dog, 13.0ms\n","Speed: 3.5ms preprocess, 13.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 17.2ms\n","Speed: 3.5ms preprocess, 17.2ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 16.4ms\n","Speed: 4.6ms preprocess, 16.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 11.1ms\n","Speed: 3.6ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 21.0ms\n","Speed: 7.9ms preprocess, 21.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 10.8ms\n","Speed: 6.2ms preprocess, 10.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 17.0ms\n","Speed: 8.4ms preprocess, 17.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 12.0ms\n","Speed: 4.2ms preprocess, 12.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 horse, 10.8ms\n","Speed: 3.0ms preprocess, 10.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 25.4ms\n","Speed: 3.3ms preprocess, 25.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 18.2ms\n","Speed: 15.4ms preprocess, 18.2ms inference, 7.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 43.1ms\n","Speed: 7.2ms preprocess, 43.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 21.6ms\n","Speed: 10.5ms preprocess, 21.6ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 14.7ms\n","Speed: 6.9ms preprocess, 14.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 52.4ms\n","Speed: 9.4ms preprocess, 52.4ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 17.8ms\n","Speed: 6.1ms preprocess, 17.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 23.3ms\n","Speed: 13.1ms preprocess, 23.3ms inference, 10.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 65.7ms\n","Speed: 3.9ms preprocess, 65.7ms inference, 7.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 handbag, 31.9ms\n","Speed: 10.2ms preprocess, 31.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 horse, 55.4ms\n","Speed: 18.2ms preprocess, 55.4ms inference, 24.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 69.1ms\n","Speed: 12.3ms preprocess, 69.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 57.7ms\n","Speed: 9.8ms preprocess, 57.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 28.6ms\n","Speed: 10.0ms preprocess, 28.6ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 73.9ms\n","Speed: 9.1ms preprocess, 73.9ms inference, 7.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 63.6ms\n","Speed: 13.8ms preprocess, 63.6ms inference, 9.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 39.7ms\n","Speed: 20.4ms preprocess, 39.7ms inference, 13.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 50.4ms\n","Speed: 7.8ms preprocess, 50.4ms inference, 12.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 26.6ms\n","Speed: 4.1ms preprocess, 26.6ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 39.4ms\n","Speed: 24.4ms preprocess, 39.4ms inference, 8.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 32.0ms\n","Speed: 9.5ms preprocess, 32.0ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 22.4ms\n","Speed: 7.3ms preprocess, 22.4ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 11.3ms\n","Speed: 5.2ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 32.3ms\n","Speed: 3.3ms preprocess, 32.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 21.6ms\n","Speed: 8.3ms preprocess, 21.6ms inference, 19.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 27.1ms\n","Speed: 3.2ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 traffic light, 10.7ms\n","Speed: 6.9ms preprocess, 10.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 28.5ms\n","Speed: 5.2ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Video saved as output_results.mp4\n"]}]},{"cell_type":"code","source":["from IPython.display import HTML\n","from base64 import b64encode\n","\n","def display_video(video_path):\n","    with open(video_path, \"rb\") as f:\n","        video = f.read()\n","    video_encoded = b64encode(video).decode(\"ascii\")\n","    return HTML(f\"\"\"\n","    <video width=\"600\" controls>\n","        <source src=\"data:video/mp4;base64,{video_encoded}\" type=\"video/mp4\">\n","    </video>\n","    \"\"\")\n","\n","# Display the video\n","display_video(output_video)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321,"output_embedded_package_id":"15IPpEgFuWHCaw4d8oOJJWePyVIO8tTcb"},"id":"F4MdCSeMSuFZ","executionInfo":{"status":"ok","timestamp":1733997377928,"user_tz":-330,"elapsed":13193,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}},"outputId":"76171b1d-7d59-459f-c5b3-975249d17faf"},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import files\n","\n","# Specify the output video file path\n","output_video = 'output_results.mp4'\n","\n","# Download the video file to your local machine\n","files.download(output_video)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"4DjT0Jt-THsl","executionInfo":{"status":"ok","timestamp":1733997399071,"user_tz":-330,"elapsed":411,"user":{"displayName":"Jahnavi Akurathi","userId":"07097912870758600565"}},"outputId":"a25540df-5091-4560-dafa-eefd21b4b311"},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_32b3d79e-f60c-4013-91a0-17c9366413d7\", \"output_results.mp4\", 8282140)"]},"metadata":{}}]}]}